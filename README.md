1.	Discuss the purpose of this database in the context of the startup, Sparkify, and their analytical goals.
2.	State and justify your database schema design and ETL pipeline.
3.	[Optional] Provide example queries and results for song play analysis.
Here's a guide on Markdown Syntax.


## 1.	Discuss the purpose of this database in the context of the startup, Sparkify, and their analytical goals.##

### This is a database contains six tables ragarding the songs, users, artists, user using time, and song play information. The purpose of the database is to help the startup stay organized and let it be able to access the data easily. As a streaming service, Sparkify has a ton of streaming data coming as their users use the services provided by Sparkify. This database normalized two datasets into six tables. The benefits of the normalization process include: more flexibility in tuning queries, easier for sorting and searching, minimize data redundacy, etc. The analytical goals of the company can include: associate similar forms of the sane data item into a single data form. We can refer the process of building the database as OLTP. ###


## 2.	State and justify your database schema design and ETL pipeline. ##
### The database schema is star schema. We can see that for the table 'songplay', it contains the columns songplay_id, user_id, song_id, artist_id, session_id, which connect the table songs, users, and artists. Compared to snowflake schema, star schema does not allow for one to many relationships while the snowflake schema does. The more detailed information of database schema is as below: \ songplays - records in log data associated with song plays i.e. records with page NextSong\ songplay_id (INT) PRIMARY KEY: ID of each user song play  \start_time (DATE) NOT NULL: Timestamp of beggining of user activity\user_id (INT) NOT NULL: ID of user\ level (TEXT): User level {free | paid}\ song_id (TEXT) NOT NULL: ID of Song played \ artist_id (TEXT) NOT NULL: ID of Artist of the song played \ session_id (INT): ID of the user Session \ location (TEXT): User location \ user_agent (TEXT): Agent used by user to access Sparkify platform \ Dimension Tables \\      users - users in the app \ user_id (INT) PRIMARY KEY: ID of user \ first_name (TEXT) NOT NULL: Name of user \ last_name (TEXT) NOT NULL: Last Name of user \ gender (TEXT): Gender of user {M | F} \ level (TEXT): User level {free | paid} \\ songs - songs in music database \ song_id (TEXT) PRIMARY KEY: ID of Song \ title (TEXT) NOT NULL: Title of Song \ artist_id (TEXT) NOT NULL: ID of song Artis \ year (INT): Year of song release \ duration (FLOAT) NOT NULL: Song duration in milliseconds \\ artists - artists in music database \ artist_id (TEXT) PRIMARY KEY: ID of Artist \ name (TEXT) NOT NULL: Name of Artist \ location (TEXT): Name of Artist city \ lattitude (FLOAT): Lattitude location of artist \ longitude (FLOAT): Longitude location of artist \\ time - timestamps of records in songplays broken down into specific units \ start_time (DATE) PRIMARY KEY: Timestamp of row \ hour (INT): Hour associated to start_time \ day (INT): Day associated to start_time \ week (INT): Week of year associated to start_time \ month (INT): Month associated to start_time \ year (INT): Year associated to start_time \ weekday (TEXT): Name of week day associated to start_timeFor the ETL pipeline, I firstly wrote a function named "get_files" to get the files' paths from the file source. Then I used the pandas package from Python to read the JSON files to load the datasets. After that, I extracted the data from datasets to create the six tables in the database. ###
